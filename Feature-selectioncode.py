# --------------
import pandas as pd
from sklearn import preprocessing

#path : File path

# Code starts here


# read the dataset
dataset = pd.read_csv(path)


# look at the first five columns
dataset.head()

# Check if there's any column which is not useful and remove it like the column id
dataset.drop(['Id'], 1, inplace = True)
dataset.describe()






# --------------
# We will visualize all the attributes using Violin Plot - a combination of box and density plots
import seaborn as sns
from matplotlib import pyplot as plt

#names of all the attributes 
cols= dataset.columns

#number of attributes (exclude target)
size= len(cols)-1

#x-axis has target attribute to distinguish between classes
x= cols[size]

#y-axis shows values of an attribute
y= cols[0:size]

#Plot violin for all attributes
for i in range(0, size):
    sns.violinplot(data = dataset, x = x, y = y[i])
    plt.show()







# --------------
import numpy
upper_threshold = 0.5
lower_threshold = -0.5


# Code Starts Here
subset_train= dataset.iloc[:, :10]
data_corr= subset_train.corr()
sns.heatmap(data_corr)
plt.show()
correlation = data_corr.unstack().sort_values(kind = 'quicksort')

corr_var_list = correlation[(correlation > upper_threshold) | (correlation < lower_threshold)]
corr_var_list = corr_var_list[corr_var_list != 1]

print(corr_var_list)

# Code ends here




# --------------
#Import libraries 
from sklearn.model_selection import train_test_split as tts
from sklearn import cross_validation
from sklearn.preprocessing import StandardScaler
import numpy as np
# Identify the unnecessary columns and remove it 
r, c = dataset.shape
dataset.drop(columns=['Soil_Type7', 'Soil_Type15'], inplace=True)
X= dataset.drop(['Cover_Type'],1)
y= dataset['Cover_Type']

X_train, X_test, y_train, y_test = tts(X, y, test_size = 0.2, random_state = 0)


# Scales are not the same for all variables. Hence, rescaling and standardization may be necessary for some algorithm to be applied on it.
scaler= StandardScaler()


#Standardized
#Apply transform only for continuous data
X_train_temp= scaler.fit_transform(X_train.iloc[:, :10])
X_test_temp= scaler.transform(X_test.iloc[:, :10])

#Concatenate scaled continuous data and categorical
X_train1 = np.concatenate((X_train_temp, X_train.iloc[:, 10:c-1]), 1)
X_test1 = np.concatenate((X_test_temp, X_test.iloc[:, 10:c-1]), 1)

scaled_features_train_df = pd.DataFrame(X_train1, columns = list(X_train), index = X_train.index)
scaled_features_test_df = pd.DataFrame(X_test1, columns = list(X_test), index = X_test.index)

print(scaled_features_train_df)
print(scaled_features_test_df)



# --------------
from sklearn.feature_selection import SelectPercentile
from sklearn.feature_selection import f_classif


# Write your solution here:
skb = SelectPercentile(score_func = f_classif, percentile = 90)

#Fit skb on training data
predictors = skb.fit_transform(X_train1, y_train)

#List of scores generated by model
scores = list(skb.scores_)

#List of all features
Features = X_train.columns

#Create a dataframe of features and theri scores
dataframe = pd.DataFrame(list(zip(Features, scores)), columns = ['Features', 'scores'])

#Sort dataframe in descending order
dataframe = dataframe.sort_values(by = 'scores', ascending = False)

#Features falling in 90th percentile
top_k_predictors = list(dataframe['Features'][:predictors.shape[1]])
print(top_k_predictors)




# --------------
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score

lr = LogisticRegression()
clf = OneVsRestClassifier(lr)
clf1 = OneVsRestClassifier(lr)

#Fitting model on training data
model_fit_all_features = clf1.fit(X_train, y_train)

#Prediction using X_test
predictions_all_features = clf1.predict(X_test)

#Accuracy Score of OneVsRest classifier using all features
score_all_features = accuracy_score(y_test, predictions_all_features)
print(score_all_features)

#Fitting model on training data containing only top features
model_fit_top_features = clf.fit(scaled_features_train_df[top_k_predictors], y_train)

#Prediction using scaled_features_test_df
predictions_top_features = clf.predict(scaled_features_test_df[top_k_predictors])

#Accuracy Score of OneVsRest classifier using top features
score_top_features = accuracy_score(y_test, predictions_top_features)
print(score_top_features)










